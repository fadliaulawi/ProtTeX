# GPU Commands - ESM-2 Tokenizer Pipeline

Complete guide for running on GPU nodes with SLURM.

---

## ðŸš€ Quick Start (Recommended)

### Run Full Pipeline (All 4 Steps)

```bash
cd /lustrefs/shared/mohammad.sayeed/Prot2Text/esmfold_tokenizer
sbatch run_gpu_pipeline.sh
```

**What it does:**
- âœ… Fetches 100 proteins from ProteinLMBench
- âœ… Extracts ESM-2 embeddings (1280-dim)
- âœ… Trains k-means codebook (512 clusters)
- âœ… Shows tokenization demo

**Time:** ~1 hour on A100, ~2 hours on V100  
**Resources:** 1 GPU, 64GB RAM, 8 CPUs

---

## ðŸ“‹ Individual Steps (Advanced)

### Step 1: Fetch Data (No GPU needed)

```bash
cd /lustrefs/shared/mohammad.sayeed/Prot2Text/esmfold_tokenizer
python 01_fetch_sample_data.py
```

**Time:** 5-10 minutes  
**Output:** `data/sample_proteins.json`

---

### Step 2: Extract ESM-2 Embeddings (GPU REQUIRED)

```bash
# Interactive GPU session
srun --partition=gpu --gres=gpu:1 --mem=32G --time=1:00:00 --pty bash

cd /lustrefs/shared/mohammad.sayeed/Prot2Text/esmfold_tokenizer
python 02_extract_esm_embeddings.py
```

**Or submit as job:**
```bash
sbatch --partition=gpu --gres=gpu:1 --mem=32G --time=1:00:00 \
  --wrap="cd /lustrefs/shared/mohammad.sayeed/Prot2Text/esmfold_tokenizer && python 02_extract_esm_embeddings.py"
```

**Time:** 30-45 minutes  
**Output:** `data/esm_embeddings.npy` (~80 MB)

---

### Step 3: Train k-means Codebook (CPU OK, but faster on GPU node)

```bash
cd /lustrefs/shared/mohammad.sayeed/Prot2Text/esmfold_tokenizer
python 03_train_kmeans_codebook.py
```

**Time:** 5-10 minutes  
**Output:** 
- `data/structure_codebook_K512.pkl`
- `data/clustering_visualization.png`

---

### Step 4: Tokenization Demo (GPU REQUIRED for ESM-2)

```bash
# Interactive GPU session
srun --partition=gpu --gres=gpu:1 --mem=16G --time=30:00 --pty bash

cd /lustrefs/shared/mohammad.sayeed/Prot2Text/esmfold_tokenizer
python 04_tokenize_and_demo.py
```

**Time:** 5-10 minutes  
**Output:** Demonstration of tokenization results

---

## ðŸ” Monitoring Jobs

### Check Job Status
```bash
squeue -u $USER
```

### View Job Output (real-time)
```bash
tail -f slurm_esm_tokenizer_*.out
```

### Check GPU Usage (in job)
```bash
nvidia-smi
watch -n 1 nvidia-smi  # Live monitoring
```

---

## ðŸ“Š Expected Results

### After Step 1 (Data):
```
data/
â”œâ”€â”€ sample_proteins.json (100 proteins)
â”œâ”€â”€ sequences.txt (FASTA format)
â””â”€â”€ metadata.txt
```

### After Step 2 (Embeddings):
```
data/
â”œâ”€â”€ esm_embeddings.npy (~80 MB, shape: [~8000, 1280])
â””â”€â”€ embedding_metadata.json
```

### After Step 3 (Codebook):
```
data/
â”œâ”€â”€ structure_codebook_K512.pkl (~13 MB)
â”œâ”€â”€ codebook_summary_K512.json
â””â”€â”€ clustering_visualization.png
```

### After Step 4 (Demo):
```
Console output showing:
- Tokenization of 3 example proteins
- Structure token assignments
- LLM input format
- Usage instructions
```

---

## âš™ï¸ Resource Requirements

### Minimal (100 proteins):
- **GPU:** 1x any CUDA GPU (8GB+ VRAM)
- **RAM:** 32GB
- **CPUs:** 4
- **Time:** ~1 hour
- **Storage:** ~200 MB

### Recommended (100 proteins):
- **GPU:** 1x A100 or V100
- **RAM:** 64GB
- **CPUs:** 8
- **Time:** 30-45 minutes
- **Storage:** ~500 MB (with visualizations)

### For Full Dataset (10K+ proteins):
- **GPU:** 1x A100 (40GB)
- **RAM:** 128GB
- **CPUs:** 16
- **Time:** 4-6 hours
- **Storage:** ~50 GB

---

## ðŸ› Troubleshooting

### Problem: "CUDA out of memory"
**Solution:**
```bash
# Reduce number of samples
# Edit 01_fetch_sample_data.py, line 13:
NUM_SAMPLES = 50  # Instead of 100
```

### Problem: "Model download timeout"
**Solution:**
```bash
# Pre-download model on login node
python -c "from transformers import EsmModel; EsmModel.from_pretrained('facebook/esm2_t33_650M_UR50D')"
```

### Problem: "Job pending too long"
**Solution:**
```bash
# Check queue
squeue

# Try different partition
sbatch --partition=gpu-long run_gpu_pipeline.sh

# Or request specific GPU
sbatch --gres=gpu:a100:1 run_gpu_pipeline.sh
```

### Problem: "datasets library not found"
**Solution:**
```bash
pip install datasets huggingface-hub transformers torch biopython scikit-learn matplotlib tqdm
```

---

## ðŸŽ¯ Validation Checklist

After running, verify:

```bash
cd /lustrefs/shared/mohammad.sayeed/Prot2Text/esmfold_tokenizer/data

# âœ“ Check data
ls -lh sample_proteins.json
python -c "import json; data=json.load(open('sample_proteins.json')); print(f'Proteins: {len(data)}')"

# âœ“ Check embeddings
ls -lh esm_embeddings.npy
python -c "import numpy as np; e=np.load('esm_embeddings.npy'); print(f'Shape: {e.shape}')"

# âœ“ Check codebook
ls -lh structure_codebook_K512.pkl
python -c "import pickle; c=pickle.load(open('structure_codebook_K512.pkl','rb')); print(f'Clusters: {c[\"n_clusters\"]}')"

# âœ“ Check visualization
ls -lh clustering_visualization.png
file clustering_visualization.png
```

**Expected:**
- âœ… sample_proteins.json exists (~200 KB)
- âœ… esm_embeddings.npy shape: [~8000, 1280]
- âœ… codebook has 512 clusters
- âœ… visualization is valid PNG image

---

## ðŸ“ˆ Performance Tips

### Speed up embeddings extraction:
```python
# Edit 02_extract_esm_embeddings.py
# Use smaller model for testing:
MODEL_NAME = "facebook/esm2_t30_150M_UR50D"  # 3x faster
```

### Speed up k-means:
```python
# Edit 03_train_kmeans_codebook.py
# Reduce max_iter:
max_iter=50  # Instead of 100
```

### Process more proteins:
```python
# Edit 01_fetch_sample_data.py
NUM_SAMPLES = 500  # Or 1000, etc.
```

---

## ðŸ”„ Rerunning Specific Steps

### Rerun embeddings only:
```bash
# Delete old embeddings
rm data/esm_embeddings.npy data/embedding_metadata.json

# Rerun
sbatch --partition=gpu --gres=gpu:1 --wrap="cd /lustrefs/shared/mohammad.sayeed/Prot2Text/esmfold_tokenizer && python 02_extract_esm_embeddings.py"
```

### Rerun k-means with different K:
```bash
# Edit 03_train_kmeans_codebook.py
# Change N_CLUSTERS = 256  # or 1024

python 03_train_kmeans_codebook.py
```

---

## ðŸ“ž Getting Help

### Check logs:
```bash
# SLURM output
cat slurm_esm_tokenizer_*.out

# Python errors
cat slurm_esm_tokenizer_*.err

# Last 50 lines
tail -50 slurm_esm_tokenizer_*.out
```

### Test individual components:
```bash
# Test imports
python -c "import torch; print(torch.cuda.is_available())"
python -c "from transformers import EsmModel; print('OK')"
python -c "from datasets import load_dataset; print('OK')"

# Test GPU
python -c "import torch; print(torch.cuda.get_device_name(0))"
```

---

## ðŸŽ“ Understanding the Pipeline

```
1. Fetch Data (01_*.py)
   â†“
   ProteinLMBench â†’ 100 proteins with sequences + descriptions
   
2. Extract Embeddings (02_*.py)
   â†“
   ESM-2 650M â†’ 1280-dim vector per residue (~8K residues total)
   
3. Train Codebook (03_*.py)
   â†“
   k-means â†’ 512 clusters = 512 "structure tokens"
   
4. Tokenize & Demo (04_*.py)
   â†“
   Each residue â†’ assigned to 1 of 512 structure tokens
   Shows how to use for LLM training
```

---

## âœ… Success Criteria

You're done when:

1. âœ… All 4 scripts run without errors
2. âœ… `data/structure_codebook_K512.pkl` exists
3. âœ… Cluster utilization > 95%
4. âœ… Visualization shows clear clustering
5. âœ… Demo shows tokenization working

**Then you're ready to scale to full dataset and integrate with LLM!**




