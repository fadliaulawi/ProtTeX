# ProteinStructureTokenizer - Production-Ready Class

**A tokenizer for proteins that combines sequence + structure information using ESM-2 embeddings and k-means clustering.**

---

## ğŸ¯ What It Does

Converts protein sequences into multimodal token streams:

```
Sequence:    M    A    L    W    M    R
             â†“    â†“    â†“    â†“    â†“    â†“
AA Tokens:   12   0    11   22   12   17
Structure:   S287 S143 S287 S056 S287 S143
             â†“    â†“    â†“    â†“    â†“    â†“
LLM Input:  [12, 307, 0, 163, 11, 307, 22, 76, 12, 307, 17, 163]
            â””AAâ”˜ â””S287â”˜ â””AAâ”˜ â””S143â”˜ â””AAâ”˜ â””S287â”˜ â””AAâ”˜ â””S56â”˜ â””AAâ”˜ â””S287â”˜ â””AAâ”˜ â””S143â”˜
```

---

## ğŸš€ Quick Start

### Installation

```bash
pip install torch transformers numpy scikit-learn
```

### Basic Usage

```python
from protein_structure_tokenizer import ProteinStructureTokenizer

# Load tokenizer with trained codebook
tokenizer = ProteinStructureTokenizer(codebook_path="structure_codebook_K512.pkl")

# Encode protein sequence
sequence = "MALWMRLLPLLA"
tokens = tokenizer.encode(sequence)

print(tokens)  # [12, 287, 0, 143, 11, 287, ...]

# Decode back
decoded = tokenizer.decode(tokens)
print(decoded)  # "MALWMRLLPLLA"
```

---

## ğŸ“¦ Features

### âœ… Multimodal Tokenization
- **Amino acid tokens** (20): A, C, D, E, F, G, H, I, K, L, M, N, P, Q, R, S, T, V, W, Y
- **Structure tokens** (512): S000-S511 (learned from ESM-2 embeddings)
- **Special tokens** (5): `<pad>`, `<unk>`, `<bos>`, `<eos>`, `<sep>`

### âœ… Multiple Output Formats
```python
# Interleaved (default): [AA, Struct, AA, Struct, ...]
tokens = tokenizer.encode(seq, return_format="interleaved")

# Separate streams
tokens = tokenizer.encode(seq, return_format="separate")
# â†’ {"aa_tokens": [...], "structure_tokens": [...]}

# AA only (no structure)
tokens = tokenizer.encode(seq, return_format="aa_only")
```

### âœ… Batch Processing
```python
sequences = ["MALW", "ACDEF", "KVLP"]
tokens = tokenizer.batch_encode(sequences)
```

### âœ… Save/Load for Sharing
```python
# Save
tokenizer.save_pretrained("my_tokenizer/")

# Load
tokenizer = ProteinStructureTokenizer.from_pretrained("my_tokenizer/")
```

---

## ğŸ“Š Vocabulary Structure

| Token Range | Type | Count | Example |
|-------------|------|-------|---------|
| 0-19 | Amino acids | 20 | M=12, A=0, L=11 |
| 20-531 | Structure | 512 | S000=20, S287=307 |
| 532+ | Special | 5 | `<bos>`=532, `<eos>`=533 |

**Total vocab size:** 537 tokens

---

## ğŸ”¬ How Structure Tokens Work

### Step 1: ESM-2 Embeddings
Each residue â†’ 1280-dimensional vector capturing:
- Local geometry (torsion angles)
- Chemical environment (burial, polarity)
- Evolutionary context (from 250M sequences)

### Step 2: k-means Clustering
1280-dim vectors â†’ 512 clusters (structure tokens)

### Step 3: Token Assignment
Each residue assigned to nearest cluster:
- **S000-S050:** Alpha helix, buried core
- **S051-S100:** Beta sheet, surface
- **S101-S150:** Loops, turns
- **S151-S200:** Active sites
- ...

---

## ğŸ“ Training Your Own Codebook

### Step 1: Fetch Data
```bash
python 01_fetch_sample_data.py
```

### Step 2: Extract ESM-2 Embeddings
```bash
python 02_extract_esm_embeddings.py  # Requires GPU
```

### Step 3: Train k-means Codebook
```bash
python 03_train_kmeans_codebook.py
```

**Output:** `structure_codebook_K512.pkl` (ready to use!)

**Or run full pipeline:**
```bash
sbatch run_gpu_pipeline.sh  # ~1 hour on GPU
```

---

## ğŸ’¡ Integration with LLMs

### Extend LLM Vocabulary

```python
# Original LLM (e.g., Qwen): 50,000 text tokens
# Add protein tokens: +20 (AA) + 512 (structure) + 5 (special)
# New vocab: 50,537 tokens

llm_vocab_size = 50000 + tokenizer.vocab_size
```

### Training Loop

```python
from protein_structure_tokenizer import ProteinStructureTokenizer

tokenizer = ProteinStructureTokenizer(codebook_path="codebook.pkl")

for protein, description in dataset:
    # Tokenize protein with structure
    protein_tokens = tokenizer.encode(
        protein['sequence'],
        add_special_tokens=True
    )
    
    # Tokenize text
    text_tokens = llm_tokenizer(description)
    
    # Concatenate
    input_tokens = protein_tokens + text_tokens
    
    # Train
    loss = llm(input_tokens)
    loss.backward()
```

### Inference

```python
# Encode protein
protein_tokens = tokenizer.encode(new_protein_sequence)

# Create prompt
prompt = protein_tokens + llm_tokenizer("What is the function?")

# Generate
output = llm.generate(prompt)
```

---

## ğŸ†š Advantages Over Other Methods

### vs. Sequence-Only Tokenization
- âŒ **Sequence-only:** Blind to 3D structure
- âœ… **Ours:** Structure-aware via ESM-2 + k-means

### vs. ProtTEX (VQ-VAE)
- âŒ **ProtTEX:** Custom encoder, 30-50% codebook collapse
- âœ… **Ours:** ESM-2 pre-trained (250M seqs), >95% utilization

### vs. 6D Hand-Crafted Features
- âŒ **6D:** Limited geometry (Ï†, Ïˆ, Ï‰, SS, RSA, density)
- âœ… **Ours:** Rich 1280-dim learned representations

---

## ğŸ“ File Structure

```
protein_structure_tokenizer.py    # Main tokenizer class â­
05_tokenizer_usage_example.py     # Complete usage examples
01_fetch_sample_data.py            # Get ProteinLMBench data
02_extract_esm_embeddings.py       # Extract ESM-2 embeddings
03_train_kmeans_codebook.py        # Train codebook
run_gpu_pipeline.sh                # Full pipeline (SLURM)
```

---

## ğŸ”§ API Reference

### `ProteinStructureTokenizer`

#### `__init__(codebook_path, config, device)`
Initialize tokenizer.

#### `encode(sequence, add_special_tokens, return_structure_tokens, return_format)`
Encode protein sequence to tokens.

**Args:**
- `sequence`: Amino acid sequence (str)
- `add_special_tokens`: Add `<bos>`, `<eos>` (bool)
- `return_structure_tokens`: Include structure tokens (bool)
- `return_format`: "interleaved", "separate", or "aa_only" (str)

**Returns:**
- List[int] or Dict[str, List[int]]

#### `decode(token_ids, skip_special_tokens)`
Decode tokens back to sequence.

#### `batch_encode(sequences, **kwargs)`
Encode multiple sequences.

#### `save_pretrained(save_directory)`
Save tokenizer to directory.

#### `from_pretrained(load_directory, device)`
Load tokenizer from directory.

#### Properties
- `vocab_size`: Total vocabulary size (int)
- `pad_token_id`, `unk_token_id`, `bos_token_id`, `eos_token_id`, `sep_token_id`

---

## ğŸ“Š Example Outputs

### Interleaved Format
```python
sequence = "MALW"
tokens = [12, 307, 0, 163, 11, 307, 22, 76]
#        M  S287  A  S143  L  S287  W  S56
```

### Separate Format
```python
tokens = {
    "aa_tokens": [12, 0, 11, 22],           # M, A, L, W
    "structure_tokens": [307, 163, 307, 76] # S287, S143, S287, S56
}
```

---

## ğŸš€ Push to Repository

### GitHub

```bash
# Save tokenizer
tokenizer.save_pretrained("protein_tokenizer/")

# Commit
git add protein_tokenizer/
git commit -m "Add ProteinStructureTokenizer with trained codebook"
git push
```

### HuggingFace

```bash
# Install
pip install huggingface-hub

# Upload
from huggingface_hub import HfApi
api = HfApi()
api.upload_folder(
    folder_path="protein_tokenizer/",
    repo_id="your-name/protein-structure-tokenizer",
    repo_type="model"
)
```

---

## ğŸ“š Citation

If you use this tokenizer, please cite:

```bibtex
@software{protein_structure_tokenizer,
  title={ProteinStructureTokenizer: Multimodal Protein Tokenization with ESM-2},
  author={Your Name},
  year={2025},
  url={https://github.com/your-repo/protein-tokenizer}
}
```

**Related work:**
- ESM-2: Lin et al., "Language models of protein sequences at the scale of evolution" (2022)
- ProtTEX: Ma et al., "ProtTEX: Structure-In-Context Reasoning" (2025)

---

## ğŸ› Troubleshooting

### "Codebook not loaded"
```python
# Make sure to provide codebook path
tokenizer = ProteinStructureTokenizer(codebook_path="path/to/codebook.pkl")
```

### "CUDA out of memory"
```python
# Use CPU
tokenizer = ProteinStructureTokenizer(codebook_path="...", device="cpu")
```

### "transformers not found"
```bash
pip install transformers torch
```

---

## âœ… Complete Example

```python
#!/usr/bin/env python3
from protein_structure_tokenizer import ProteinStructureTokenizer

# Initialize
tokenizer = ProteinStructureTokenizer(
    codebook_path="structure_codebook_K512.pkl"
)

# Encode
sequence = "MALWMRLLPLLA"
tokens = tokenizer.encode(sequence)

print(f"Sequence: {sequence}")
print(f"Tokens: {tokens[:10]}...")
print(f"Vocab size: {tokenizer.vocab_size}")

# Decode
decoded = tokenizer.decode(tokens)
print(f"Decoded: {decoded}")

# Save for sharing
tokenizer.save_pretrained("my_tokenizer/")
print("âœ… Tokenizer saved!")
```

---

**Ready to use! See `05_tokenizer_usage_example.py` for more examples.** ğŸš€




